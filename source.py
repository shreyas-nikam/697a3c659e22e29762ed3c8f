import hashlib
import uuid
import datetime
import json
import pandas as pd
# IPython.display is removed from core module imports as it's a UI/notebook-specific dependency.
# It can be re-imported in __main__ block for demonstration or handled by the consuming app.
LAB_VERSION = "1.0"
EXPORT_FORMAT_VERSION = "lab1_export_v1"

REQUIRED_FIELDS = [
    "model_name",
    "business_use",
    "domain",
    "model_type",
    "deployment_mode",              # ADD THIS
    "decision_criticality",
    "data_sensitivity",
    "automation_level",
    "regulatory_materiality",
]
# Define the scoring logic for inherent risk factors
# Each factor's categories are mapped to points
RISK_SCORING_TABLE = {
    'decision_criticality': {
        'Low': 1,
        'Medium': 3,
        'High': 5
    },
    'data_sensitivity': {
        'Public': 1,
        'Internal': 2,
        'Confidential': 3,
        'Regulated-PII': 5
    },
    'automation_level': {
        'Manual': 1,
        'Human-in-the-loop': 3,
        'Fully-Automated': 5
    },
    'regulatory_materiality': {
        'None': 1,
        'Low': 2,
        'Medium': 4,
        'High': 5
    }
}

DOMAIN_OPTIONS = [
    "Operations Efficiency", "Credit Risk", "Market Risk",
    "Fraud Detection", "Customer Segmentation", "Compliance", "Other"
]
MODEL_TYPE_OPTIONS = [
    "ML classifier (time-series)", "Regression", "Decision Tree",
    "Neural Network", "Statistical (e.g., ARIMA)", "Expert Rule-based", "Other"
]
DEPLOYMENT_MODE_OPTIONS = ["Real-time", "Batch", "Offline Analysis"]

# Define the thresholds for risk tiers
# Lower score means lower risk tier (e.g., Tier 3 is lowest risk)
TIER_THRESHOLDS = {
    "Tier 1": {"min_score": 16, "description": "High Risk: Significant impact, requires extensive MRM oversight."},
    "Tier 2": {"min_score": 9,  "description": "Medium Risk: Moderate impact, requires standard MRM oversight."},
    "Tier 3": {"min_score": 0,  "description": "Low Risk: Minimal impact, well-understood, or highly controlled."},
}

SCORING_VERSION = "v1.0"


def scoring_config_snapshot() -> dict:
    return {
        "scoring_version": SCORING_VERSION,
        "risk_scoring_table": RISK_SCORING_TABLE,
        "tier_thresholds": TIER_THRESHOLDS,
    }


def scoring_config_hash(snapshot: dict) -> str:
    payload = json.dumps(snapshot, sort_keys=True)
    return hashlib.sha256(payload.encode("utf-8")).hexdigest()


def register_model_metadata(model_details: dict) -> dict:
    """
    Registers the model metadata, including generating audit fields.
    Validates required fields and controlled vocabularies.

    Args:
        model_details (dict): A dictionary containing the model's core metadata.

    Returns:
        dict: The complete model registration record with audit fields.

    Raises:
        ValueError: If required fields are missing/empty or if invalid values
                    are provided for risk factors.
    """
    if model_details.get("domain") not in DOMAIN_OPTIONS:
        raise ValueError(
            f"Invalid domain '{model_details.get('domain')}'. Allowed: {', '.join(DOMAIN_OPTIONS)}")
    if model_details.get("model_type") not in MODEL_TYPE_OPTIONS:
        raise ValueError(
            f"Invalid model_type '{model_details.get('model_type')}'. Allowed: {', '.join(MODEL_TYPE_OPTIONS)}")
    if model_details.get("deployment_mode") not in DEPLOYMENT_MODE_OPTIONS:
        raise ValueError(
            f"Invalid deployment_mode '{model_details.get('deployment_mode')}'. Allowed: {', '.join(DEPLOYMENT_MODE_OPTIONS)}")

    # Validate required fields
    required_fields = REQUIRED_FIELDS
    for field in required_fields:
        if field not in model_details or not model_details[field]:
            raise ValueError(f"Required field '{field}' is missing or empty.")

    # Enforce controlled vocabularies for risk factors
    for factor, allowed_values in RISK_SCORING_TABLE.items():
        if factor in model_details and model_details[factor] not in allowed_values:
            raise ValueError(f"Invalid value '{model_details[factor]}' for '{factor}'. "
                             f"Allowed values are: {', '.join(allowed_values.keys())}")

    # Create a copy to avoid modifying the original input dict directly
    registered_details = model_details.copy()

    # Generate model_id if not provided (assume it's generated by the system)
    if 'model_id' not in registered_details or not registered_details['model_id']:
        registered_details['model_id'] = str(uuid.uuid4())

    # Add audit fields
    registered_details['created_at'] = datetime.datetime.now(
        datetime.timezone.utc).isoformat()
    registered_details['created_by'] = registered_details.get(
        'created_by', 'System/Unknown Owner')  # Allow override, default if not provided
    registered_details['lab_version'] = '1.0'  # Or make this dynamic if needed

    return registered_details


def calculate_inherent_risk(model_metadata: dict, scoring_table: dict, tier_thresholds: dict, scoring_version: str) -> dict:
    """
    Calculates the inherent risk score and proposed tier for a model based on its metadata.

    Args:
        model_metadata (dict): The registered model's metadata, typically output from register_model_metadata.
        scoring_table (dict): The predefined scoring logic for risk factors.
        tier_thresholds (dict): The predefined thresholds for risk tiers.
        scoring_version (str): The version of the scoring logic used.

    Returns:
        dict: A dictionary containing the inherent risk score, proposed tier,
              proposed tier description, and score breakdown.
    """
    inherent_risk_score = 0
    score_breakdown = {}

    for factor, score_map in scoring_table.items():
        if factor in model_metadata:
            value = model_metadata[factor]
            # Ensure the value exists in the score_map, though register_model_metadata should prevent this
            if value in score_map:
                points = score_map[value]
                inherent_risk_score += points
                score_breakdown[factor] = {'value': value, 'points': points}
            else:
                score_breakdown[factor] = {'value': value, 'points': 0,
                                           'warning': 'Value not found in scoring map for this factor.'}
        else:
            score_breakdown[factor] = {'value': 'N/A', 'points': 0,
                                       'warning': f'Factor "{factor}" not in model metadata, cannot score.'}

    proposed_risk_tier = 'Undefined'
    tier_description = 'No tier assigned or thresholds not met.'
    # Sort tiers by max_score to ensure correct assignment for overlapping ranges, if any.
    # In this case, Tier 3 -> Tier 2 -> Tier 1 is already ordered by max_score ascending.
    sorted_tiers = sorted(tier_thresholds.items(),
                          key=lambda item: item[1]["min_score"], reverse=True)
    for tier, data in sorted_tiers:
        if inherent_risk_score >= data["min_score"]:
            proposed_risk_tier = tier
            tier_description = data["description"]
            break

    config = scoring_config_snapshot()

    return {
        'inherent_risk_score': inherent_risk_score,
        'proposed_risk_tier': proposed_risk_tier,
        'proposed_tier_description': tier_description,
        'score_breakdown': score_breakdown,
        'scoring_config': config,
        'scoring_version': scoring_version
    }


def assess_model_risk(raw_model_details: dict) -> dict:
    """
    Orchestrates the model registration and inherent risk calculation process.
    This is the primary function to be called by an application.

    Args:
        raw_model_details (dict): A dictionary containing the model's core metadata
                                  as initially provided by the user.

    Returns:
        dict: The complete model record, including audit fields and
              inherent risk assessment results.

    Raises:
        ValueError: If required fields are missing, empty, or if invalid values
                    are provided for risk factors during registration.
    """
    try:
        # 1. Register the model metadata and add audit fields
        registered_model = register_model_metadata(raw_model_details)

        # 2. Calculate the inherent risk score and tier
        risk_assessment = calculate_inherent_risk(
            registered_model,
            RISK_SCORING_TABLE,
            TIER_THRESHOLDS,
            SCORING_VERSION
        )

        # 3. Update the registered model record with the risk assessment results
        registered_model.update(risk_assessment)

        return registered_model

    except ValueError as e:
        # Re-raise validation errors for the calling application to handle
        raise e
    except Exception as e:
        # Catch any other unexpected errors during the process
        raise RuntimeError(
            f"An unexpected error occurred during model risk assessment: {e}")
